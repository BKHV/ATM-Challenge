{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 43.54 MB\n",
      "Memory usage after optimization is: 10.65 MB\n",
      "Decreased by 75.6%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "import sys,random\n",
    "#def main(argv):\n",
    "\n",
    "#Great snippet from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        #else:\n",
    "        #    df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "alldata = reduce_mem_usage(pd.read_csv('train.csv',sep=';'))\n",
    "alldata['is_test'] = np.random.choice([True,False],size=len(alldata))\n",
    "\n",
    "columns = ['cash_in_out','display_type','scanner_code_reader','atm_id']\n",
    "alldata.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "\n",
    "categorical_cols = [col for col in alldata.select_dtypes(include=[\"object\"]).columns]\n",
    "\n",
    "#Mean encoding of categorical variables\n",
    "for col in categorical_cols:\n",
    "    means = alldata.loc[~alldata.is_test, :].groupby(col)[\"target\"].mean()\n",
    "    alldata.loc[:, \"%s_MEAN\" % col] = alldata.loc[:, col].map(means)\n",
    "    \n",
    "    #Missing values is filled with global mean\n",
    "    alldata.loc[:, \"%s_MEAN\" % col] = alldata.loc[:, \"%s_MEAN\" % col].fillna(means.mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "alldata.loc[:, categorical_cols] = alldata.loc[:, categorical_cols].apply(lambda x: LabelEncoder().fit_transform(x.astype(str))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1: 0.60823654\n",
      "Fold 2 F1: 0.64033264\n",
      "Fold 3 F1: 0.61169102\n",
      "F1 score 0.60473208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cols_to_drop = [\"target\", \"is_test\"]\n",
    "X_train = alldata.loc[~alldata.is_test, :].drop(cols_to_drop, axis=1)\n",
    "y_train = alldata.loc[~alldata.is_test, \"target\"]\n",
    "\n",
    "X_test = alldata.loc[alldata.is_test, :].drop(cols_to_drop, axis=1)\n",
    "\n",
    "n_splits = 3\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros(X_train.shape[0])\n",
    "\n",
    "sub = pd.read_csv(\"sample_submission.csv\")\n",
    "sub[\"target\"] = 0\n",
    "feature_importances = pd.DataFrame()\n",
    "\n",
    "for i, (fit_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    \n",
    "    X_fit = X_train.iloc[fit_idx]\n",
    "    y_fit = y_train.iloc[fit_idx]\n",
    "    X_val = X_train.iloc[val_idx]\n",
    "    y_val = y_train.iloc[val_idx]\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "        max_depth=3,\n",
    "        num_leaves=5 ** 2 - 1,\n",
    "        learning_rate=0.003,\n",
    "        n_estimators=3000,\n",
    "        min_child_samples=80,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=1,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=0\n",
    "        #,random_state=np.random.randint(10e6)\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "        eval_set=[(X_fit, y_fit), (X_val, y_val)],\n",
    "        eval_names=('fit', 'val'),\n",
    "        eval_metric='binary_logloss',\n",
    "        early_stopping_rounds=150,\n",
    "        verbose=False\n",
    "    )\n",
    "      \n",
    "    oof_preds[val_idx] = model.predict_proba(X_val, num_iteration=model.best_iteration_)[:, 1]\n",
    "    #sub['target'] += model.predict_proba(X_test, num_iteration=model.best_iteration_)[:,1]\n",
    "    \n",
    "    fi = pd.DataFrame()\n",
    "    fi[\"feature\"] = X_train.columns\n",
    "    fi[\"importance\"] = model.feature_importances_\n",
    "    fi[\"fold\"] = (i+1)\n",
    "    \n",
    "    feature_importances = pd.concat([feature_importances, fi], axis=0)\n",
    "    \n",
    "    y_pred=oof_preds[val_idx]\n",
    "    y_pred=y = [1 if i >=np.mean(y_pred) else 0 for i in y_pred]\n",
    "    \n",
    "    print(\"Fold {} F1: {:.8f}\".format(i+1, f1_score(y_val, y_pred)))\n",
    "\n",
    "y_pred=oof_preds\n",
    "y_pred=y = [1 if i >=np.mean(y_pred) else 0 for i in y_pred]\n",
    "print('F1 score %.8f' % f1_score(y_train, y_pred))   \n",
    "    \n",
    "#sub[\"TARGET\"] /= n_splits\n",
    "#sub.to_csv(\"lgbm.csv\", index=None, float_format=\"%.8f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
